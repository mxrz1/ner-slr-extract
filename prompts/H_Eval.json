{
  "block": "H_Eval",
  "features": [
    {
      "name": "Cost evaluation",
      "question": "Is computational cost analysed? How measured?",
      "guideline": "If „Yes“, then describe how it was measured and present the results."
    },
    {
      "name": "Quantitative results",
      "question": "Which evaluation metrics are reported **and** for which evaluation task (experiment) was each metric used?  \nProvide **all evaluation results** (all metric values), and state the **task/experiment** where each value was obtained.",
      "guideline": "- **Two-pass extraction:**  \n  1) **Identify** all evaluation tasks/experiments (dataset, split, subset, setting, model variant).  \n  2) **Collect** every reported **metric → value** for each identified experiment.\n- **Per-experiment separation:** **Do not merge** results across different tasks/datasets/splits. Keep a clear **per-experiment** mapping.\n- **Metric semantics:** For each metric, capture **averaging** (micro/macro), **span level** (token/entity), and **matching policy** (exact/partial/strict) when stated.\n- **Metrics (illustrative, not limiting):** Precision, Recall, F1 (micro/macro), accuracy, exact-match, span-F1, entity-F1, AUC, etc.\n- **Answer priority:** In **Answer**, list **best test-set scores per metric for the final/best CTI-NER model** first. Include dataset/split/experiment names with each top-line score.\n- **All values required:** In the **Long explanation**, include **every reported value** for **every experiment**, not just the best ones (typically via the exported tables below).\n- **Table→Experiment mapping (bullets):** Before the exported tables, add a bullet list where each entry includes: **Table label** (e.g., “Table 2” or a synthetic ID), **Experiment tags** (dataset, split, subset, model variant), and **Association confidence** (high/medium/low).  \n  - **Association method (in order):** use **table caption**; else **nearest heading in the same section**; else **inline references** (e.g., “see Table 2”).  \n  - If ambiguous, include candidate tags and mark **“caption/association uncertain.”**\n- **Export tables (verbatim):** Under **“Exported evaluation tables”**, insert the **Markdown tables exactly as retrieved from the chunks**. **Do not reformat** or summarize. If a **caption** is found in an adjacent/linked chunk, place it immediately **above** the table as an *italic* line.\n- **Captions as evidence:** **Table captions** (when present as plain text in chunks) **may be quoted as evidence**. **Figure content remains excluded.**\n- **No inference of numbers:** Do **not** fabricate or estimate metric values. Only record values explicitly stated in body text or Markdown tables.\n\n**Output alignment**  \nConform **exactly** to the global per-field format (**Answer; Explanation & key points; Long explanation / description; Evidence**).\n\n**Expected structure inside Long explanation (Markdown)**  \n1. **Experiment catalog** (bullets): dataset, split (train/dev/test), subset, model variant (final/best vs baseline), notes.  \n2. **Table→Experiment mapping** (bullets as specified above).  \n3. **Exported evaluation tables** (verbatim from chunks; captions placed above when found).  \n4. **Baselines / ablations** (if any): list values per experiment with evidence.  \n5. **Notes:** clarify averaging/span/matching definitions when stated; otherwise `Not reported`. If a caption–table association was ambiguous, mark **“caption/association uncertain.”**"
    },
    {
      "name": "Interpretation",
      "question": "What conclusions do the authors explicitly draw from their evaluation results? Based on the reported results, what additional conclusions can reasonably be inferred, even if not directly stated by the authors?",
      "guideline": "- **Two-track structure (no mixing):** Split output into **Author-stated conclusions** and **Model-inferred conclusions**. Keep them separate.\n- **Qualitative emphasis:** Capture the authors’ **interpretations**, not just numbers: explanations, error sources, trade-offs, limitations, generalization/transfer claims, dataset effects, ablation takeaways.\n- **Evidence (authors):** For each **Author-stated** conclusion, include a **verbatim quote** (≤120 chars) from **body text or Markdown tables**.\n- **Inference hygiene (model):** For each **Model-inferred** conclusion, add:\n  - **Premises:** specific results/ablations/splits/metrics referenced\n  - **Reasoning:** 1–2 lines connecting premises → conclusion\n  - **Confidence:** low / medium / high\n  - *(No quotes in inferred items; quotes are for Author-stated only.)*\n- **Ablation linkage:** Tie inferences to **named experiments/variants** (e.g., “+CRF vs −CRF on test micro-F1”).\n- **Conflict & scope:** If results conflict across datasets/splits, **state the conflict** and **constrain** the conclusion’s scope accordingly.\n- **Language fidelity:** Preserve author hedging/uncertainty (e.g., “suggests”, “may improve”). Do **not** strengthen cautious claims.\n- **Second-pass recall:** Sweep **Results, Discussion, Conclusion, Error Analysis, Limitations**, then perform a **second pass** to catch missed qualitative points.\n\n**Answer format (concise)**\n\n- **Author-stated conclusions (top 3–5):** short bullets; each with a **verbatim quote** (≤120 chars) and a parenthetical **pointer to the experiment** (dataset/split/metric).\n- **Model-inferred conclusions (1–3):** short bullets with **Premises**, **Reasoning (1–2 lines)**, **Confidence**.\n\n**Output alignment**  \n\nConform **exactly** to the global per-field format (**Answer; Explanation & key points; Long explanation / description; Evidence**).\n\n- **Answer:** Provide the two bullet lists above.\n- **Explanation & key points:** One sentence summarizing overarching themes (e.g., where gains come from; where it fails).\n- **Long explanation / description:** Full catalog of author-stated conclusions (with context), followed by full model-inferred conclusions with **Premises/Reasoning/Confidence**; include any conflicts/limitations explicitly.\n- **Evidence:** For **Author-stated** items, provide the quotes used in **Answer** (and any additional supportive quotes). **Do not** include quotes for inferred items.\n"
    },
    {
      "name": "Evaluation strategy",
      "question": "How is the evaluation strategy designed?",
      "guideline": "Describe what experiments (evaluation tasks) have been done to evaluate the model. This includes (but not limited to) the high level tests, folding, splitting into train/validation/test sets, ablation studies, significance tests."
    }
  ]
}