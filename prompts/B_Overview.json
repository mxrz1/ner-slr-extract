{
  "block": "B_Overview",
  "features": [
    {
      "name": "Generic Overview",
      "question": "Give an overview of the paper.",
      "guideline": "Structure it between Motivation, Overview, Preprocessing, Architecture, Evaluation results, findings, Dataset."
    },
    {
      "name": "Description",
      "question": "Describe the **NER architecture or approach** presented in this paper.",
      "guideline": "**Extraction question**  \nDescribe the **NER architecture** presented in this paper.\n\n**Guideline**  \nProvide a **sequential, but flexible** overview using the paper’s own stage names. Explain how each part works by itself and how it integrates with the others. Include the **model name** if the authors provided one.\n\n**Scope**  \nDescribe **NER architecture only**. **Exclude** evaluation metrics, training/implementation details (hardware, schedules), dataset-selection procedures, and relevance/observation classifiers **unless** they directly define the NER architecture.\n\n**NER subsystem isolation rule**  \nExtract and describe the **NER subsystem only** (representation → encoder(s) → tagger/decoder → post-processing). Treat upstream/downstream modules (e.g., retrieval, relevance filtering/classifiers, relation extraction, summarization) as **out of scope**. Mention them once in **“Context (out of scope)”** only if needed for orientation; **do not** describe their internals.\n\n**Component disambiguation**  \nIf a component appears in multiple roles (e.g., BiLSTM), create **separate entries** with **role tags** (e.g., `BiLSTM[char-level]`, `BiLSTM[sequence]`). **Do not merge** these roles.\n\n**Module card (expanded detail)**  \nFor **each NER component**, write a **short paragraph (≥3 sentences)** covering:  \n1) **Inputs & interface** (shapes/types if stated)  \n2) **Mechanics/algorithm** (how it works)  \n3) **Purpose** (authors’ rationale)  \n4) **Wiring** (how it connects/fuses with prior/next modules)  \n5) **Key settings** (only if reported)\n\nIf authors use only a generic name (e.g., “contextualized embeddings”), keep it and add a clarifier via **`[Background note] <≤25 words>`**. Background notes are **explanatory only** (not evidence).\n\n**Final-architecture rule**  \nIf multiple variants exist, describe **only the final/best NER model** used for the main results.  \nAdd a subsection **“Not selected variants”** listing NER components/combos **tested but excluded**.  \nIf “best” is not explicit, choose the model tied to **primary results** or named the **main/full model**.\n\n**Evidence mapping**  \nProvide at least **one quote per module** (≤120 chars) from **body text or Markdown tables**. Do **not** quote figures/“Key insights”.\n\n**Context (out of scope)**  \nOptionally list non-NER pipeline elements (e.g., retrieval, relevance classifier, relation extraction) in a simple bullet list **without** internal details.\n\n**Output alignment**  \nConform **exactly** to the global per-field format (**Answer; Explanation & key points; Long explanation / description; Evidence**).  \n- **Answer:** Start with the **NER-only sequence** line.  \n- **Explanation & key points:** One sentence summary + concise bullets highlighting the core NER blocks and their interactions.  \n- **Long explanation / description:** The **module cards** (one per NER component), plus **Not selected variants** and (optional) **Context (out of scope)**.  \n- **Evidence:** Per-module quotes (and any additional supporting quotes), drawn only from body text or Markdown tables.\n",
      "model": "gpt-5-mini",
      "reasoning_effort": "medium"
    },
    {
      "name": "High-level architecture description",
      "question": "Describe the sequence of components used in the approach/architecture given the format defined in the guideline.",
      "guideline": "Mixed approaches process data through components arranged in stages. Specify the NER architecture as a **single-line Sequence** using **dashes** between stages and **plus signs (“+”)** within a stage. The canonical stages are:\n\n1. **Embeddings**\n2. **Model** (or **Encode**)\n3. **NER extraction** (or **Decode**) — include **post-processing** here via `+` (e.g., `CRF+Rules`)\n\n- **Sequence line (mandatory):** The **very first line** of **Answer** must be the Sequence.  \n  - If a stage is truly absent, write **`None`** for that stage; if unspecified, mark **`Not reported`**.\n\n**NER subsystem isolation rule**  \nThe Sequence must be **NER-only** (representations → encoder(s) → tagger/decoder → post-processing). Upstream/downstream modules (e.g., retrieval, RAG, relevance filters/classifiers, relation extraction, summarization) are **out of scope**. If needed for orientation, list them once under **“Context (out of scope)”** without internal detail.\n\n**Bracket annotations (required for each component token)**  \nAfter **each component token**, add parentheses with the concrete mechanism/instantiation, e.g.:  \n- `characterEmbedding(CNN)`, `wordEmbedding(GloVe-6B)`, `contextualizedEmbeddings(BERT-base-uncased)`, `BiLSTM(2-layer)`, `Self-Attention(scaled-dot)`, `CRF(linear-chain)`, `rules(Regex,Gazetteer)`  \nIf authors use a **generic name** (e.g., “contextualized embeddings”), keep it **and** add a clarifier showing **how it is created and what concepts are behind it**.  \nIf the bracketed mechanism is **not stated** in the paper and comes from general knowledge, append `*` to that bracket item (e.g., `contextualizedEmbeddings(LanguageModel*)`). Provide a matching **`[Background note]`** (≤25 words) in *Explanation & key points* and expand in *Long explanation*. Background items are **not evidence**.\n\n**Component disambiguation & role tags**  \nIf a component appears in multiple roles, add **role tags** and list them separately (e.g., `BiLSTM[sequence](2-layer)` vs `BiLSTM[char](1-layer)`). **Do not merge** roles.\n\n**Creation method (concise)**  \nFor **each component**, state **how it is realized** (e.g., *char embeddings via CNN*; *subword via BPE/WordPiece; contextual via named LM; decoder via CRF; rules via regex/gazetteer*).\n\n**Module wiring (one-to-two-line summary)**  \nIn the *Long explanation*, briefly explain **how modules connect/fuse** (concat, residual, attention fusion, stacking) and **what each module contributes**. Keep this concise to preserve the high-level focus.\n\n**Final-architecture rule**  \nIf multiple variants exist, the Sequence must reflect **only the final/best NER model** used for main results.  \nAdd **“Not selected variants”** listing other NER component combos **tested but excluded** (one bullet each).\n\n**Evidence mapping (concise)**  \nProvide at least **one quote per stage** (or per ambiguous token) to verify the presence/instantiation of named components. Quotes must come from **body text or Markdown tables** (≤120 chars). Do **not** quote figures/“Key insights”.\n\n**Context (out of scope)**  \nOptionally list non-NER pipeline elements (retrieval, RAG, relevance classifier, relation extraction) in a simple bullet list **without** internal details.\n\n**Output alignment**  \nFollow the global per-field format (**Answer; Explanation & key points; Long explanation / description; Evidence**).  \n- **Answer:** Begin with the **NER-only Sequence** (dash/plus with brackets).  \n- **Explanation & key points:** One sentence summary; include any **`[Background note]`** entries referenced by `*`.  \n- **Long explanation / description:** 1–2 paragraphs on creation methods and wiring; add **Not selected variants** and optional **Context (out of scope)**.  \n- **Evidence:** Minimal quotes that prove the components named in the Sequence.\n- **No preamble:** Apply all steps internally; do **not** print any plan/progress text.\n- **Two-pass is internal:** Do **not** echo the “Two-pass extraction” steps in the output.\n- **Answer-first rule:** The response must begin with `- **Answer:** …` (best test-set scores if available; else `Not reported`).\n- **Empty-case policy:** If no metrics/tables are present in the provided chunks:\n  - Set **Answer = `Not reported`**.\n  - In **Long explanation** add:\n    - **Experiment catalog:** `None found in provided chunks`\n    - **Table→Experiment mapping:** `None`\n    - **Exported evaluation tables:** `None`\n    - **Notes:** brief reason (e.g., only figures available; captions absent).",
      "model": "gpt-5-mini",
      "reasoning_effort": "medium"
    }
  ]
}