{
  "block": "C_Components",
  "features": [
    {
      "name": "Rule-based components",
      "question": "Are rule-based components a part of the proposed approach? If so, name/describe them.",
      "guideline": "- **Answer-first:** Begin **Answer** with `Yes / No / Not reported — <one-line summary>`, e.g., `Yes — regex patterns + vendor gazetteer for organization entities`.\n- **Final-first rule:** In **Answer**, list **rule-based components used by the final/best CTI-NER model first**; place others under **“Not selected / auxiliary”** (with evidence).\n- **Synonym normalization:** Treat *rules/heuristics/patterns/templates*, *dictionary/lexicon/gazetteer/whitelist/blacklist*, *ontology mapping*, *labeling functions (LFs)* as **rule-based** when deterministic.\n- **Not algorithms (scope boundary):** **Exclude deterministic algorithms** (e.g., **Aho–Corasick**, **KMP**, **Rabin–Karp**, **deterministic tokenizers**). Those belong to **Non-ML deterministic algorithms**. This feature covers **handcrafted/programmatically generated rules** and **lookups**.\n- **Illustrative, not limiting:** regex/pattern templates; gazetteers/lexicons (vendor lists, CWE/CVE-derived); ontology-based mapping (e.g., STIX label → entity type); dictionary lookups; rule engines; **Snorkel labeling functions**.\n- **Role tags (where used):** Tag each component with its locus: `[preprocess]`, `[extraction]`, `[post]`.\n- **Describe each component (when stated):**\n  - **What it matches** (entity types/strings/pattern families)\n  - **Conflict resolution** (priority, longest-match, fallback)\n  - **Coverage/precision notes** (e.g., “high precision for vendor names”)\n  - **Source/provenance** (gazetteer/ontology origin, update policy)\n- **Generated rules:** If rules are **machine-induced** (e.g., CNN/LM creates regex, Snorkel LFs), note the **generator** and **selection criteria**.\n- **Inference-time vs data-only:** If rules are used **only** for augmentation/weak labeling, state **“Labeling/augmentation only (not used at inference)”**.\n- **Evidence:** Provide **≥1 verbatim quote per rule component** (≤120 chars).\n- **Background note (optional):** If a gazetteer/ontology is named but not described, add **`[Background note] <≤25 words>`** (explanatory only; not evidence).\n- **No preamble:** Apply steps internally; **do not** print plans/progress.\n\n**Output alignment**  \nConform **exactly** to the global per-field format (**Answer; Explanation & key points; Long explanation / description; Evidence**).  \n- **Answer:** `Yes/No/Not reported — <summary>`, then a concise, comma-separated list of final/best rule components with **role tags**, e.g.,  \n  `regex[extraction] (IOC patterns), gazetteer[preprocess] (vendor list), ontology-mapping[post] (STIX→entity)`\n- **Explanation & key points:** One sentence on **why** rules are used and where; add 3–5 bullets only if helpful (conflict policy, coverage, provenance).\n- **Long explanation / description:** Per component: role, what it matches, conflict policy, coverage/precision notes, provenance, whether generated or handcrafted, and whether used at inference or only for labeling. Include **Not selected / auxiliary** rules if present.\n- **Evidence:** One quote **per component** (≤120 chars) ",
      "model": "gpt-5-nano",
      "reasoning_effort": "medium"
    },
    {
      "name": "Algorithms",
      "question": "Are there algorithms used, that do not involve machine learning, transformers, large language models, or deep learning?",
      "guideline": "- **Definition (deterministic algorithms only):** Report **named, deterministic algorithms** (no ML/LLM/Transformer/deep learning).\n- **Explicit exclusions:** **Exclude** statistical/probabilistic or learned models (e.g., **CRF, HMM, SVM, logistic regression**), **Transformers/LLMs**, and generic **rule-based heuristics** (regex/gazetteers) unless a **specific algorithm** is named. Generic “rules/regex patterns” belong to **Rules & Gazetteers**, not here.\n- **Tokenizers included (deterministic):** Include **deterministic tokenizers** (e.g., **spaCy rule-based tokenizer**, **NLTK WordPunctTokenizer**). **Exclude learned subword tokenizers** (e.g., **BPE, WordPiece**).\n- **Illustrative, not limiting:** **Aho–Corasick**, **KMP**, **Rabin–Karp**, **Trie-based exact matching**, **deterministic tokenizers**, **Porter/Snowball stemmer**, **WordNet lemmatizer**, **finite-state segmenters**.\n- **Per-algorithm gloss (mandatory):** After each algorithm name in **Answer**, add a **3–8 word** plain-English gloss of **what it does** in parentheses (e.g., `Aho–Corasick (fast multi-pattern string matching)`). If the gloss isn’t stated in the paper, add a **`[Background note]`** in *Explanation & key points* (non-evidential).\n- **What to capture (per algorithm):** **Name → Pipeline stage** (preprocess/NER extraction/post-processing) → **What it does** (one line) → **Inputs → Outputs** → **Key settings** (if stated).",
      "model": "gpt-5-nano",
      "reasoning_effort": "medium"
    },
    {
      "name": "Statistical learners",
      "question": "Are statistical learners a part of the proposed approach? If so, name/describe them and their peculiarities.",
      "guideline": "**Guideline**  \n\n- **Final-first rule:** In **Answer**, list **statistical learners used by the final/best CTI-NER model first**. Place learners **tested but not used** under **“Not selected learners”** (with evidence).\n- **Include CRF when it’s the decoder:** If a **CRF** is used on top of a neural encoder as the **NER decoder**, it **counts as a statistical learner**. List it here (e.g., `CRF[decoder] (linear-chain, Viterbi)`).\n- **Role tagging:** If the same learner appears in multiple roles, add a role tag, e.g., `SVM[filter]`, `CRF[decoder]`, `MaxEnt[tagger]`.\n- **Synonym normalization (recognition):** Treat **Maximum Entropy / MaxEnt / log-linear**, **CRF / conditional random field (linear-chain)**, **HMM / n-th order HMM**, **SVM / support vector machine**, **Perceptron / averaged perceptron**, **Naive Bayes**, **Logistic regression** as **equivalent names**.\n- **Illustrative, not limiting:** CRF (linear-chain/higher-order), HMM, MaxEnt/log-linear, SVM, Perceptron, Naive Bayes, Logistic regression; **TF-IDF/BM25** when used as **features or candidate generator** for NER (otherwise see *Context*).\n- **Peculiarities to capture (when stated):** order (CRF/HMM), feature templates, decoding (Viterbi/beam), kernel & C (SVM), regularization, calibration, class weighting, constraints, **history-based** tagging (MaxEnt).\n- **Scope boundary:** **Exclude** deep neural modules and LMs/Transformers. Include **TF-IDF/BM25** **only if** they **feed the NER learner**; otherwise list them once under **Context (out of scope)** without details.\n- **Evidence:** Provide **≥1 verbatim quote per learner** (≤120 chars) from **body text or Markdown tables**. No figures/captions.\n- **Consistency rule:** If any learners are evidenced, **Answer must not be `Not reported`**. If none are evidenced, set **Answer = `Not reported`** and list nothing.\n\n**Output alignment**  \nConform **exactly** to the global per-field format (**Answer; Explanation & key points; Long explanation / description; Evidence**).  \n- **Answer:** concise, comma-separated list with brief clarifiers, e.g.,  \n  `CRF[decoder] (linear-chain, first-order; Viterbi), MaxEnt[tagger] (history-based), SVM[filter] (RBF kernel)`  \n- **Explanation & key points:** One sentence summarizing which learners are **in the final model**, plus 3–5 bullets for key peculiarities (if helpful).  \n- **Long explanation / description:** For each learner: role, inputs/features, peculiarities (order, kernel, decoding, constraints), and how it integrates with the NER pipeline. Add **Not selected learners** (if any).  \n- **Evidence:** One quote **per learner** from body text/Markdown tables.\n\n**Context (out of scope)**  \nIf TF-IDF/BM25 or other scoring is used **only** for retrieval unrelated to NER, mention it here in one bullet without details.\n",
      "model": "gpt-5-nano",
      "reasoning_effort": "medium"
    },
    {
      "name": "Prompt engineering",
      "question": "Does the approach employ natural language inputs/outputs to extract entities? If so, describe the prompt design.",
      "guideline": "- **Answer-first summary:** Start **Answer** with `Yes / No / Not reported — <one-line prompt style>`, e.g., `Yes — few-shot instruction prompt with JSON schema`.\n- **Exhaustive listing (final-first):** If multiple prompts are used, **list all**. Put **final/best inference prompts first**; move others to **“Not selected / auxiliary prompts”** (e.g., augmentation, weak labeling, evaluation checker).\n- **Prompt taxonomy:** Classify as applicable: **Inference**, **Training/finetune templates**, **Augmentation/labeling**, **Retrieval-augmented (RAG)**, **Evaluation/checker**, **Post-processing/validation**.\n- **Design facets to capture (when stated):**  \n  - **Purpose** (what the prompt achieves)  \n  - **Template structure** (slots/variables, question templates)  \n  - **Entity schema encoding** (label list, verbalizers, definitions)  \n  - **Format constraints** (JSON schema, tag spans, delimiter tokens)  \n  - **Example count** (k-shot; positions of exemplars)  \n  - **Instruction style** (instructional vs Q/A vs generative extraction)  \n  - **Decoding params** (temperature/top-p/max tokens/stop)  \n  - **Context sources** (RAG retrieval, section-limited context)  \n  - **Safety/guardrails** (bans, refusal handling)  \n  - **Prompt generation** (handcrafted, programmatic, auto-generated)\n- **Example prompt policy:** If an **example prompt** is provided, include it (or **multiple excerpts ≤120 chars** each).\n- **Scope boundary:** Count **only natural-language inputs/outputs** used for **entity extraction** or its immediate setup. **Exclude** bare regex/rules unless they are part of an NL prompt; exclude UI strings/config keys.\n- **Disambiguation:** If style is **not explicitly named**, **infer from usage** and append **`(inferred from components)`**; support with quotes for the premises.\n- **Not inference-time:** If prompts are used **only for augmentation/labeling**, state **“Augmentation only (not used at inference)”**.\n- **Evidence:** Provide **≥1 verbatim quote per prompt type** (≤120 chars) from **body text or Markdown tables**.\n- **No preamble:** Apply any multi-pass reasoning **internally**; **do not** print plans or progress.\n\n**Output alignment**  \nConform **exactly** to the global per-field format (**Answer; Explanation & key points; Long explanation / description; Evidence**).\n\n**Expected content**\n- **Answer:** `Yes/No/Not reported — <one-line prompt style>` and, if `Yes`, a concise list of prompt types used by the **final/best** model first (e.g., `Inference: few-shot JSON; RAG: section-limited`).\n- **Explanation & key points:** One sentence summarizing **what the prompts do** and **why**; bullets (if helpful) for schema, examples, constraints, decoding params.\n- **Long explanation / description:**  \n  - **Per prompt type:** purpose, template (slots), schema encoding, examples (k-shot), constraints (JSON/tags), decoding params, context retrieval, safety/guardrails, and **how/where in the pipeline** it’s applied.  \n  - **Not selected / auxiliary prompts:** augmentation/labeling or checker prompts not used at inference.  \n  - Mark **`(inferred from components)`** where applicable.\n- **Evidence:** Short prompt excerpts (≤120 chars each) and any lines naming the prompt type/usage.\n",
      "model": "gpt-5-nano",
      "reasoning_effort": "medium"
    },
    {
      "name": "Reinforcement Learning",
      "question": "Is Reinforcement Learning (RL) applied? If so, summarise algorithm and purpose.",
      "guideline": "- **Answer-first:** Begin **Answer** with `Yes / No / Not reported — <one-line RL summary>`, e.g., `Yes — PPO fine-tuning of decoder to maximize span-F1`.\n- **RL definition (scope):** Count **only** methods with an explicit **policy**, **actions**, **rewards**, and a named **RL algorithm** (policy gradient or value-based).\n- **Synonyms (recognition boost):** Consider **REINFORCE/policy gradient**, **actor–critic (A2C/A3C)**, **PPO/TRL**, **QLearning/DQN**, **bandits**, **reward shaping**, **RLHF/RLAIF**, **DPO/KTO** as RL family **when the paper frames them as RL**.\n- **Not RL (explicit exclusions):** Exclude **self-training**, **curriculum learning**, **early stopping**, **hard-negative mining**, **loss reweighting**, **scheduled sampling** (unless explicitly cast as RL), **maximum-margin** methods, and vague “reinforcement signals” without a named RL algorithm.\n- **Final-first rule:** List RL used by the **final/best CTI-NER model** first; place other RL uses under **“Not selected / auxiliary”** (e.g., for augmentation or ablations).\n- **Where applied:** State the locus: **decoder/tagger**, **prompting** (prompt optimization), **retrieval/reranking**, **data selection/augmentation**, or **training control**. Note whether RL affects **inference** or **training only**.\n- **What to capture (when stated):**  \n  - **Algorithm & variant:** PPO/REINFORCE/A2C/DQN/bandit, etc.  \n  - **State:** token/span sequence, prompt template, candidate entities, etc.  \n  - **Action space:** tag assignment, span proposal, prompt edit, retrieval choice.  \n  - **Reward:** span-F1, exact-match, boundary reward, latency/precision trade-off, human feedback.  \n  - **Objective & purpose:** e.g., improve boundary recall, reduce hallucinations, calibrate precision.  \n  - **Regime:** on/off-policy; batch/online; key **hyperparams** (γ, clip ε, KL β) if provided.  \n  - **Integration:** where RL sits in the NER pipeline and how it interacts with other components.\n- **Evidence:** Provide **≥1 verbatim quote per RL use** (≤120 chars) naming the **algorithm** and/or **reward**. \n- **Background note (optional):** If an RL method is named without explanation, add **`[Background note] <≤25 words>`** to clarify at a high level. Background is **not evidence**.\n- **No preamble:** Apply steps **internally**; **do not** print goals/plans/progress.\n\n**Output alignment**  \nFollow the global per-field format (**Answer; Explanation & key points; Long explanation / description; Evidence**).  \n- **Answer:** `Yes/No/Not reported — <one-line RL summary>`; if `Yes`, name algorithm + purpose + locus.  \n- **Explanation & key points:** One sentence summarising **algorithm, reward, locus, purpose**; bullets only if helpful.  \n- **Long explanation / description:** Detail **algorithm**, **state/actions/reward**, **integration point**, **hyperparams** (if reported), and whether RL affects **inference**. Add **Not selected / auxiliary** RL if present.  \n- **Evidence:** Quotes that explicitly name the RL algorithm and/or reward from body text/Markdown tables.\n",
      "model": "gpt-5-nano",
      "reasoning_effort": "medium"
    },
    {
      "name": "Deep-learning components",
      "question": "Are deep-learning components a part of the proposed approach? If so, name/describe them.",
      "guideline": "- **Scope (final first):** Report deep-learning components used by the **final/best CTI-NER model first**. Then list components **tested but not used** in a short “Not selected variants” note (with evidence).\n- **Allowed (illustrative, not limiting):** **CNN/Char-CNN**, **(Bi)LSTM**, **GRU**, **vanilla RNN**, **MLP/FFN heads**, **attention modules attached to RNN/CNN** (additive/dot-product), **gating/fusion networks**.\n- **Explicit exclusions:** **Exclude** all **Transformer-based** models/layers (e.g., **BERT, RoBERTa, DeBERTa, GPT, Transformer encoders/decoders, self-attention stacks**), **LLMs/PLMs**, and **pretrained LM embeddings**. Also **exclude non-deep** components (e.g., **CRF, HMM, SVM, rules, gazetteers**).  \n  - If the paper states **BiLSTM-CRF**, **report BiLSTM only** here; CRF is out of scope.  \n  - Transformer/LM items belong to the **Embeddings/features** feature and must **not** appear here.\n- **Synonyms:** Treat *neural network*, *sequence model*, *recurrent model* as **equivalent** to deep components for extraction.\n- **Role tagging:** If a component appears in multiple roles, add a role tag, e.g., `BiLSTM[sequence]`, `CNN[char]`.\n- **Creation/realization:** When the paper explains **how** a component is realized, include it in parentheses, e.g., `CNN[char] (conv-k3)`, `BiLSTM (2-layer)`, `Attention (additive)`.",
      "model": "gpt-5-nano",
      "reasoning_effort": "medium"
    }
  ]
}