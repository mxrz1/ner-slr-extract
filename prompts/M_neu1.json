{
  "block": "neu1",
  "features": [
    {
      "name": "LM training regime",
      "question": "Is a pretrained language model (LM) used with the NER approach? If so, state **exactly how it is integrated and trained** (embeddings-only vs frozen encoder vs partial fine-tune vs full fine-tune) and whether **quantization** is used.",
      "guideline": "- **Answer-first (compact, standardized):**  \n  `Yes / No / Not reported — LM=<name>; Lineage=<None|DAPT|TAPT|STILTs|Other(<desc>)>; NER=<E1|E2|E3|E4>; Trainable=<none|head-only|last-N|all>; Adapters=<LoRA/QLoRA/Adapters/IA3/None>; Quantization=<int8|int4|None>(tool)`\n\n- **Axis A — Lineage / pre-adaptation (prior to NER):**  \n  - **DAPT (domain-adaptive pretraining):** extra **unsupervised LM** training on **domain corpus**.  \n  - **TAPT (task-adaptive pretraining):** extra **unsupervised LM** training on **the task’s unlabeled data**.  \n  - **STILTs / Intermediate-task training:** supervised training on a **labeled intermediate task** (e.g., NLI) prior to NER.  \n  - **Other:** describe (e.g., continued MLM on custom CTI text).  \n  *Capture:* corpus, objective (e.g., MLM), steps/epochs if stated.  \n  *If none stated:* `Lineage=None`.  \n  *(Background references: DAPT/TAPT; STILTs)*\n\n- **Axis B — NER-stage training regime (final/best model):**  \n  - **(E1) Embeddings-only (feature-based):** LM used to precompute static features; **not** in the training graph; no LM params updated.  \n  - **(E2) Encoder (frozen):** LM **in** the model; **all** LM layers frozen; train **head-only**.  \n  - **(E3) Partial fine-tune:** some LM params updated (e.g., **adapters/LoRA/QLoRA/IA3**, **last N layers**, prefix/prompt-tuning).  \n  - **(E4) Full fine-tune:** **all** LM layers trainable.  \n  - **Quantization:** note **int8/int4** and tool (e.g., **bitsandbytes**). **QLoRA** = **E3** with `Quantization=int4(bnb)` and `Adapters=QLoRA`.  \n  - **“End-to-end / jointly trained / all layers updated”** → map to **E4** only if gradients through LM are implied; otherwise classify per evidence.\n\n- **Final-first rule:** Report the **final/best CTI-NER model** first; put other experimented lineages/regimes under **“Not selected setups”**.\n\n- **Recognition cues (generic):**  \n  - *Lineage:* “continued/further pretraining,” “MLM on domain/task data,” “intermediate task (e.g., NLI) before NER.”  \n  - *NER regime:* “frozen/linear probe/head-only,” “last N layers,” “adapters/LoRA/QLoRA,” “end-to-end/jointly trained.”\n\n- **Evidence:** Provide **≥1 short quote per asserted aspect** (lineage type + corpus/objective; NER regime; adapters/layers; quantization). Quotes must be from **body text or Markdown tables** (≤120 chars). No figures/captions.\n\n- **Ambiguity handling:** If any field is unclear, set it to **`Not reported`** and briefly explain in *Long explanation*. You may add a **`[Background note] <≤25 words>`** to clarify concepts (explanatory only; not evidence).\n\n**Output alignment**  \nFollow the global per-field format (**Answer; Explanation & key points; Long explanation / description; Evidence**).\n\n**Examples (illustrative only)**  \n- `Yes — LM=BERT-base-uncased; Lineage=DAPT(MLM on CTI reports) → NER=Encoder(frozen); Trainable=head-only; Adapters=None; Quantization=None`  \n- `Yes — LM=RoBERTa-large; Lineage=STILTs(NLI) → NER=Full; Trainable=all; Adapters=None; Quantization=None`  \n- `Yes — LM=Llama-7B; Lineage=None → NER=Partial; Trainable=LoRA(r=16); Adapters=QLoRA; Quantization=int4(bnb)`"
    }
  ]
}